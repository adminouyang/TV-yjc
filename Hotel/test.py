import os
import re
import requests
import time
import concurrent.futures
import random
from datetime import datetime

# ===============================
# é…ç½®åŒº
# ===============================

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
]

# æœç´¢å…³é”®è¯ï¼ˆbase64ç¼–ç ï¼‰
SEARCH_QUERIES = [
    "ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04i",  # IPTVç›´æ’­
]

# IPå­˜å‚¨ç›®å½•
IP_DIR = "Hotel/ip"
if not os.path.exists(IP_DIR):
    os.makedirs(IP_DIR)

# ===============================
# IPå¤„ç†å‡½æ•°
# ===============================

def get_isp(ip):
    """IPè¿è¥å•†åˆ¤æ–­"""
    # ç”µä¿¡IPæ®µ
    telecom_pattern = r"^(1\.|14\.|27\.|36\.|39\.|42\.|49\.|58\.|60\.|101\.|106\.|110\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|171\.|175\.|182\.|183\.|202\.|203\.|210\.|211\.|218\.|219\.|220\.|221\.|222\.)"
    # è”é€šIPæ®µ
    unicom_pattern = r"^(42\.1[0-9]{0,2}|43\.|58\.|59\.|60\.|61\.|110\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|171\.8[0-9]|171\.9[0-9]|171\.1[0-9]{2}|175\.|182\.|183\.|210\.|211\.|218\.|219\.|220\.|221\.|222\.)"
    # ç§»åŠ¨IPæ®µ
    mobile_pattern = r"^(36\.|37\.|38\.|39\.1[0-9]{0,2}|42\.2|42\.3|47\.|106\.|111\.|112\.|113\.|114\.|115\.|116\.|117\.|118\.|119\.|120\.|121\.|122\.|123\.|124\.|125\.|126\.|134\.|135\.|136\.|137\.|138\.|139\.|150\.|151\.|152\.|157\.|158\.|159\.|170\.|178\.|182\.|183\.|184\.|187\.|188\.|189\.)"
    
    if re.match(telecom_pattern, ip):
        return "ç”µä¿¡"
    elif re.match(unicom_pattern, ip):
        return "è”é€š"
    elif re.match(mobile_pattern, ip):
        return "ç§»åŠ¨"
    else:
        return "æœªçŸ¥"

def get_ip_info(ip_port):
    """è·å–IPåœ°ç†ä¿¡æ¯"""
    try:
        ip = ip_port.split(":")[0]
        
        # ä½¿ç”¨IP-APIæŸ¥è¯¢
        try:
            response = requests.get(f"http://ip-api.com/json/{ip}?lang=zh-CN", timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "success":
                    province = data.get("regionName", "æœªçŸ¥")
                    isp = get_isp(ip)
                    return province, isp, ip_port
        except:
            pass
        
        return "æœªçŸ¥", "æœªçŸ¥", ip_port
        
    except Exception as e:
        return "æœªçŸ¥", "æœªçŸ¥", ip_port

def read_existing_ips(filepath):
    """è¯»å–ç°æœ‰æ–‡ä»¶å†…å®¹å¹¶å»é‡"""
    existing_ips = set()
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        match = re.match(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5})', line)
                        if match:
                            existing_ips.add(match.group(1))
        except Exception as e:
            pass
    return existing_ips

def generate_fofa_urls():
    """ç”ŸæˆFOFAæœç´¢URL"""
    urls = []
    pages = 5
    page_size = 20
    
    for query in SEARCH_QUERIES:
        for page in range(1, pages + 1):
            url = f"https://fofa.info/result?qbase64={query}&page={page}&page_size={page_size}"
            urls.append(url)
    
    return urls

def crawl_fofa():
    """çˆ¬å–FOFAæ•°æ®"""
    urls = generate_fofa_urls()
    all_ips = set()
    session = requests.Session()
    
    for i, url in enumerate(urls, 1):
        print(f"ğŸ“¡ æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(urls)} é¡µ...")
        
        try:
            time.sleep(random.uniform(1, 3))
            headers = {"User-Agent": random.choice(USER_AGENTS)}
            response = session.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # åŒ¹é…IP:ç«¯å£
                matches = re.findall(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5})', response.text)
                for match in matches:
                    # éªŒè¯IPæ ¼å¼
                    ip_match = re.match(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})', match)
                    if ip_match:
                        ip_parts = ip_match.group(1).split('.')
                        if all(0 <= int(part) <= 255 for part in ip_parts):
                            all_ips.add(match)
                
                print(f"âœ… ç¬¬ {i} é¡µè·å–åˆ° {len(matches)} ä¸ªIPï¼Œå½“å‰æ€»æ•° {len(all_ips)}")
            else:
                print(f"âŒ ç¬¬ {i} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ç¬¬ {i} é¡µçˆ¬å–å¤±è´¥: {e}")
    
    return all_ips

def process_and_save_ips(ip_list):
    """å¤„ç†IPå¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    if not ip_list:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°IP")
        return
    
    print(f"ğŸ”§ å¼€å§‹å¤„ç† {len(ip_list)} ä¸ªIP...")
    
    province_isp_dict = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_ip = {executor.submit(get_ip_info, ip): ip for ip in ip_list}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_ip), 1):
            province, isp, ip_port = future.result()
            
            if province and isp and isp != "æœªçŸ¥":
                province_clean = province.replace("çœ", "").replace("å¸‚", "").strip()
                if not province_clean:
                    province_clean = "æœªçŸ¥"
                fname = f"{province_clean}{isp}.txt"
                province_isp_dict.setdefault(fname, set()).add(ip_port)
            
            if i % 50 == 0 or i == len(ip_list):
                print(f"â³ å·²å¤„ç† {i}/{len(ip_list)} ä¸ªIP...")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    for fname, ips in province_isp_dict.items():
        filepath = os.path.join(IP_DIR, fname)
        existing_ips = read_existing_ips(filepath)
        new_ips = ips - existing_ips
        
        if new_ips:
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(f"\n# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                for ip in sorted(new_ips):
                    f.write(ip + '\n')
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(new_ips)} ä¸ªæ–°IPåˆ° {fname}")
    
    print(f"âœ… IPå¤„ç†å®Œæˆï¼å…±ä¿å­˜åˆ° {len(province_isp_dict)} ä¸ªåˆ†ç±»æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ğŸŒ IPåœ°å€æŠ“å–å·¥å…·")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {IP_DIR}")
    print("=" * 50)
    
    print("\nğŸš€ å¼€å§‹çˆ¬å–FOFAæ•°æ®...")
    all_ips = crawl_fofa()
    
    if all_ips:
        print(f"\nğŸ¯ æ€»å…±è·å–åˆ° {len(all_ips)} ä¸ªIP")
        process_and_save_ips(all_ips)
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•IPåœ°å€")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print("=" * 50)

if __name__ == "__main__":
    main()
